# Default values for byconity.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
# please replace  storageClassName
nameOverride: ""
fullnameOverride: ""

byconity:
  image: byconity/byconity:stable
  imagePullPolicy: IfNotPresent
  hdfs_address: hdfs://byconity-hdfs-namenodes:8020 # can using your own hdfs
  #  use_existing_fdb: true
  #fdb_cluster_file: byconity_fdb:Is0hBgl6iICdHuspBmhAODmD5WISXKzI@192.168.224.150:4501,192.168.226.83:4501,192.168.228.152:4501
  ports:
    tcp: 9000
    http: 8123
    rpc: 8124
    tcpSecure: 9100
    https: 9123
    exchange: 9410
    exchangeStatus: 9510

  usersOverwrite:
    users:
      default:
        password: ""
      probe:
        password: probe

  server:
    replicas: 2
    image: ""
    podAnnotations: { }
    resources: { }
    hostNetwork: false
    nodeSelector: { }
    tolerations: [ ]
    affinity: { }
    imagePullSecrets: [ ]
    securityContext: { }
    livenessProbe:
      exec:
        command: ["/opt/byconity/scripts/lifecycle/liveness"]
      failureThreshold: 6
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 20
    readinessProbe:
      exec:
        command: ["/opt/byconity/scripts/lifecycle/readiness"]
      failureThreshold: 5
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    storage:
      localDisk:
        pvcSpec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 30Gi
          storageClassName: openebs-hostpath #replace to your storageClassName
      log:
        pvcSpec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
          storageClassName: openebs-hostpath #replace to your storageClassName
    configOverwrite:
      logger:
        level: trace
      disk_cache_strategies:
        simple:
          lru_max_size: 429496729600 # 400Gi
      # timezone: Etc/UTC
    hdfs3Config:
      dfs.client.metrics.enable: true
      dfs.client.metrics.use_domain_socket: true
      dfs.default.blocksize: "268435456"
      dfs.read.hedge_read.interval_ms: 120
      input.connect.timeout: 10000
      input.read.max.retry: 3
      input.read.timeout: 6000
      input.write.timeout: 10000
      output.close.timeout: 256000
      output.connect.timeout: 10000
      output.default.write.retry: 3
      output.read.timeout: 120000
      output.write.timeout: 256000
      rpc.client.connect.retry: 3
      rpc.client.connect.timeout: 10000
      rpc.client.read.timeout: 20000
      rpc.client.timeout: 20000
      rpc.client.write.timeout: 20000

  tso:
    replicas: 3  #Currently fixed at 3 for HA
    image: ""
    podAnnotations: { }
    resources:
      limits:
        cpu: "3"  # >=3
        memory: 2Gi
      requests:
        cpu: "3"  # >=3
        memory: 2Gi
    hostNetwork: false
    nodeSelector: { }
    tolerations: [ ]
    affinity: { }
    imagePullSecrets: [ ]
    securityContext: { }
    additionalVolumeClaimTemplates:   #add new pvc for coordination
      - metadata:
          name: coordination
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 5Gi
          storageClassName: openebs-hostpath #replace to your storageClassName

    storage:
      localDisk:
        pvcSpec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 30Gi
          storageClassName: openebs-hostpath #replace to your storageClassName
      log:
        pvcSpec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
          storageClassName: openebs-hostpath #replace to your storageClassName
    additionalVolumes:    #add new volume for coordination
      volumes:
      volumeMounts:
        - name: coordination
          mountPath: /var/byconity/tso/coordination
    configOverwrite:
      zookeeper: { } #need to add this empty tag
      keeper_server:   #configuration for HA
        server_id: 0
        tcp_port: 9181  #service byconity-tso-headless port2
        log_storage_path: /var/byconity/log/coordination
        snapshot_storage_path: /var/byconity/snapshots
        coordination_settings:
          operation_timeout_ms: 10000
          session_timeout_ms: 30000
          raft_logs_level: warning
          compress_logs: 1
        raft_configuration:
          - server:
              id: 0   #corresponds to the index of the pod. start from 0
              hostname: byconity-tso-0.byconity-tso.byconity.svc.cluster.local   # the domain of the pod.can enter the pod and ping its own to get this value
              port: 9445  #service byconity-tso-headless port1
          - server:
              id: 1
              hostname: byconity-tso-1.byconity-tso.byconity.svc.cluster.local  # the domain of the pod.can enter the pod and ping its own to get this value
              port: 9445  #service byconity-tso-headless port1
          - server:
              id: 2
              hostname: byconity-tso-2.byconity-tso.byconity.svc.cluster.local  # the domain of the pod.can enter the pod and ping its own to get this value
              port: 9445  #service byconity-tso-headless port1


  daemonManager:
    replicas: 1 # Please keep single instance now, daemon manager HA is WIP
    image: ""
    podAnnotations: { }
    resources: { }
    hostNetwork: false
    nodeSelector: { }
    tolerations: [ ]
    affinity: { }
    imagePullSecrets: [ ]
    securityContext: { }
    configOverwrite: { }

  resourceManager:
    replicas: 2
    image: ""
    podAnnotations: { }
    resources: { }
    hostNetwork: false
    nodeSelector: { }
    tolerations: [ ]
    affinity: { }
    imagePullSecrets: [ ]
    securityContext: { }
    configOverwrite: { }

  defaultWorker: &defaultWorker
    replicas: 1
    image: ""
    podAnnotations: { }
    resources: { }
    hostNetwork: false
    nodeSelector: { }
    tolerations: [ ]
    affinity: { }
    imagePullSecrets: [ ]
    securityContext: { }
    storage:
      localDisk:
        pvcSpec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 50Gi
          storageClassName: openebs-hostpath #replace to your storageClassName
      log:
        pvcSpec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
          storageClassName: openebs-hostpath #replace to your storageClassName
    configOverwrite:
      logger:
        level: trace
      disk_cache_strategies:
        simple:
          lru_max_size: 429496729600 # 400Gi
      # timezone: Etc/UTC
    hdfs3Config:
      dfs.client.metrics.enable: true
      dfs.client.metrics.use_domain_socket: true
      dfs.default.blocksize: "268435456"
      dfs.read.hedge_read.interval_ms: 120
      input.connect.timeout: 10000
      input.read.max.retry: 3
      input.read.timeout: 6000
      input.write.timeout: 10000
      output.close.timeout: 256000
      output.connect.timeout: 10000
      output.default.write.retry: 3
      output.read.timeout: 120000
      output.write.timeout: 256000
      rpc.client.connect.retry: 3
      rpc.client.connect.timeout: 10000
      rpc.client.read.timeout: 20000
      rpc.client.timeout: 20000
      rpc.client.write.timeout: 20000

  virtualWarehouses:
    - <<: *defaultWorker
      name: vw_default
      replicas: 1
    - <<: *defaultWorker
      name: vw_write
      replicas: 1

  commonEnvs:
    - name: MY_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: "metadata.namespace"
    - name: MY_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: "metadata.name"
    - name: MY_UID
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: "metadata.uid"
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: "status.podIP"
    - name: MY_HOST_IP
      valueFrom:
        fieldRef:
          # fieldPath: "status.hostIP"
          fieldPath: "status.podIP"
    - name: CONSUL_HTTP_HOST
      valueFrom:
        fieldRef:
          fieldPath: "status.hostIP"

  additionalEnvs: [ ]

  additionalVolumes:
    volumes: [ ]
    volumeMounts: [ ]

  postStart: ""
  preStop: ""
  readiness: ""
  liveness: ""

  ingress:
    enabled: false
    className: ""
    annotations: { }
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    hosts:
      - host: byconity-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: [ ]
    #  - secretName: byconity-example-tls
    #    hosts:
    #      - byconity-example.local

# For more detailed usage, please check fdb-kubernetes-operator API doc: https://github.com/FoundationDB/fdb-kubernetes-operator/blob/main/docs/cluster_spec.md
fdb:
  enabled: true
  enableCliPod: true
  version: 7.1.15
  imagePullSecrets: [ ]
  clusterSpec:
    mainContainer:
      imageConfigs:
        - { version: 7.1.15,baseImage: foundationdb/foundationdb, tag: 7.1.15 }
    sidecarContainer:
      imageConfigs:
        - { version: 7.1.15,baseImage: foundationdb/foundationdb-kubernetes-sidecar, tag: 7.1.15-1 }
    processCounts:
      stateless: 3
      log: 3
      storage: 3
    processes:
      general:
        volumeClaimTemplate:
          spec:
            storageClassName: openebs-hostpath #replace to your storageClassName
            resources:
              requests:
                storage: 20Gi
#
#    fdb:
#      clusterSpec:
#        processes:
#          general:
#            volumeClaimTemplate:
#              spec:
#                storageClassName: local-device-hdd

fdb-operator:
  enabled: true
  resources:
    limits:
      cpu: 1
      memory: 512Mi
    requests:
      cpu: 1
      memory: 512Mi
  image:
    repository: foundationdb/fdb-kubernetes-operator
    tag: v1.9.0
    pullPolicy: IfNotPresent

  imagePullSecrets: [ ]
  initContainers:
    6.2:
      image:
        repository: foundationdb/foundationdb-kubernetes-sidecar
        tag: 6.2.30-1
        pullPolicy: IfNotPresent
    6.3:
      image:
        repository: foundationdb/foundationdb-kubernetes-sidecar
        tag: 6.3.23-1
        pullPolicy: IfNotPresent
    7.1:
      image:
        repository: foundationdb/foundationdb-kubernetes-sidecar
        tag: 7.1.15-1
        pullPolicy: IfNotPresent

hdfs:
  enabled: true
  alpine:
    repository: alpine  #if your machine is amd64.please use amd64/alpine
    tag: 3.10.2
    imagePullPolicy: IfNotPresent
  namenode:
    repository: gchq/hdfs
    tag: 3.2.2 # managed version
    resources: { }
    nodeSelector: { }
    dataVolumes:
      count: 1
      pvcTemplateSpec:
        storageClassName: openebs-hostpath #replace to your storageClassName
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
    extraEnvVars: { }
    # Example usage:
    # HADOOP_HEAPSIZE_MIN: 128m
    # hadoop.heapsize.min: 128m
  datanode:
    nodeCount: 3
    repository: gchq/hdfs
    tag: 3.2.2 # managed version
    resources: { }
    nodeSelector: { }
    dataVolumes:
      count: 1
      pvcTemplateSpec:
        storageClassName: openebs-hostpath
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
    extraEnvVars: { }
    # Example usage:
    # HADOOP_HEAPSIZE_MIN: 128m
    # hadoop.heapsize.min: 128m
  config:
    hdfsSite:
      dfs.permissions.enabled: "false"
      dfs.namenode.datanode.registration.ip-hostname-check: "false"
  shell:
    enabled: true
    repository: gchq/hdfs
    tag: 3.2.2 # managed version
    imagePullPolicy: IfNotPresent
    imagePullSecrets: [ ]
    resources: { }
    nodeSelector: { }
    tolerations: [ ]
    affinity: { }
  postInstallCommands: [ ]
  # - hadoop fs -mkdir -p /accumulo
  # - hadoop fs -chown accumulo /accumulo
  # - hadoop fs -chmod 700 /accumulo
  # - hadoop fs -ls /
